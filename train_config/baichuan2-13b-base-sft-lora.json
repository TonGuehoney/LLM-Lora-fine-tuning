{
"model_path": "/nfs/model/llm/baichuan/Baichuan2-13B-Base",
"model_name": "Baichuan2-13B-Base",
"output_dir": "./path_to_sft_checkpoint/Baichuan2-13B-Chat_trained",
"dataset": "alpaca_zh",
"dataset_dir": "./data",

"finetuning_type": "lora",
"quantization_bit": 4,
"lora_rank": 64,
"lora_alpha": 16,
"lora_dropout": 0.1,

"do_train": true,
"num_train_epochs": 1,
"per_device_train_batch_size": 16,
"gradient_accumulation_steps": 4,
"cutoff_len": 1024,
"learning_rate": 2e-4,
"lr_scheduler_type": "linear",
"logging_steps": 10,
"save_strategy": "steps",
"save_steps": 20,
"save_total_limit": 2,

"warmup_steps": 50,
"overwrite_cache": true,
"overwrite_output_dir": true,
"gradient_checkpointing": true,
"report_to": "tensorboard",
"fp16": true
}